{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsSXr_a8jE9S",
        "colab_type": "text"
      },
      "source": [
        "## **Set up ML Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2TJP5K9mD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnZX3DhVD6Su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 list | grep tensorflow\n",
        "!pip3 install tensorflow==1.13.1 # the binary is still in the 1.13.1 version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOWdsRSabO2-",
        "colab_type": "code",
        "outputId": "e65bc02e-24c0-4100-c81c-89f3f66d9ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing   import LabelBinarizer\n",
        "from tensorflow              import keras\n",
        "from keras.callbacks         import ModelCheckpoint\n",
        "from keras.models            import Sequential\n",
        "from keras.layers            import Dense, Flatten, Conv2D, Dropout\n",
        "from os.path                 import join\n",
        "from keras.optimizers        import Adam\n",
        "\n",
        "import keras.backend     as K\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow        as tf\n",
        "import numpy             as np\n",
        "import pandas            as pd\n",
        "import seaborn           as sb\n",
        "import pdb, os, random, keras, re\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lihceFjcKGm",
        "colab_type": "text"
      },
      "source": [
        "## Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiUueStR0xdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xNormalize(value, max_value, min_value):\n",
        "    return (value - min_value)/(max_value - min_value)\n",
        "\n",
        "def yNormalize(value, max_value):\n",
        "    if value < 0:\n",
        "        if max_value == 0:\n",
        "            return -1\n",
        "        else:\n",
        "            # return max_value*(-1)\n",
        "            return -1\n",
        "    \n",
        "    return value\n",
        "\n",
        "def load_data2(directory):\n",
        "    all_files = [join(directory, filename) for filename in os.listdir(directory)]\n",
        "    \n",
        "    random.shuffle(all_files)\n",
        "    \n",
        "    first_file = all_files.pop(0) \n",
        "    all_data   = pd.read_csv(first_file)\n",
        "    \n",
        "    for file in all_files:\n",
        "        data = pd.read_csv(file)\n",
        "        \n",
        "        all_data = all_data.append(data)\n",
        "        \n",
        "    return all_data\n",
        "\n",
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    \n",
        "    for i in range(3):\n",
        "        data = data.sample(frac=1)\n",
        "    \n",
        "    return data\n",
        "\n",
        "def get_coordID_values(num_nodes):\n",
        "    values = []\n",
        "    \n",
        "    for i in range(1, number_of_nodes + 1):\n",
        "        values.append('CoordID_' + str(i))\n",
        "        \n",
        "    return values\n",
        "    \n",
        "def get_estimatePeerID_values(num_nodes):\n",
        "    values = []\n",
        "    \n",
        "    for i in range(1, number_of_nodes + 1):\n",
        "        values.append('EstimatePeerID_' + str(i))\n",
        "        \n",
        "    return values\n",
        "\n",
        "vote    = 20\n",
        "coordID = 1\n",
        "peerID  = 200\n",
        "maj     = 10\n",
        "deci    = 1\n",
        "\n",
        "def desnormalize(data_norm):\n",
        "    for column in data_norm.columns:\n",
        "        if ('PeerID_' in column):\n",
        "            data_norm[column] = data_norm[column].apply(lambda x: x * peerID)\n",
        "        elif ('Decision' in column):\n",
        "            data_norm[column] = data_norm[column].apply(lambda x: x * deci)    \n",
        "        elif ('Vote' in column):\n",
        "            data_norm[column] = data_norm[column].apply(lambda x: x * vote)\n",
        "        elif ('Maj' in column):\n",
        "            data_norm[column] = data_norm[column].apply(lambda x: x * maj)\n",
        "        elif ('CoordID' in column):\n",
        "            data_norm[column] = data_norm[column].apply(lambda x: x * coordID)\n",
        "            \n",
        "    return data_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EunUbwmQcJGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_nodes          = 40\n",
        "algorithm_variables      = 8\n",
        "matrix_rows, matrix_cols = 1, (algorithm_variables - 4) + number_of_nodes*4 + 2 \n",
        "\n",
        "filepath = 'sample_data/snapshots_' + str(number_of_nodes)  + '.csv'\n",
        "data     = load_data(filepath)\n",
        "\n",
        "train_set_size = int(len(data) * 0.8)\n",
        "\n",
        "data_np = data.to_numpy()\n",
        "delay   = data_np[:, (algorithm_variables + number_of_nodes):]\n",
        "\n",
        "max_val_delay = np.amax(delay)\n",
        "min_val_delay = np.amin(delay)\n",
        "\n",
        "data_norm = pd.get_dummies(data, \n",
        "                           columns=['EstimateValue', 'Decision', 'PeerID', 'Phase', 'EstimatePeerID', 'CoordID'],\n",
        "                           prefix=['EstimateValue', 'Decision', 'PeerID', 'Phase', 'EstimatePeerID', 'CoordID'])\n",
        "\n",
        "current_estimatePeerID_values = list(filter(lambda x: 'EstimatePeerID' in x, data_norm.columns.values))\n",
        "current_coordID_values        = list(filter(lambda x: 'CoordID'        in x, data_norm.columns.values))\n",
        "current_phase_value           = list(filter(lambda x: 'Phase'          in x, data_norm.columns.values))\n",
        "\n",
        "coordID_values        = get_coordID_values(number_of_nodes)\n",
        "EstimatePeerID_values = get_estimatePeerID_values(number_of_nodes)\n",
        "phase_values          = ['Phase_1', 'Phase_2']\n",
        "\n",
        "phase_colums          = data_norm.T.reindex(phase_values).T.fillna(0)\n",
        "coordID_columns       = data_norm.T.reindex(coordID_values).T.fillna(0)\n",
        "estimatePeerID_colums = data_norm.T.reindex(EstimatePeerID_values).T.fillna(0)\n",
        "\n",
        "data_norm.drop(current_estimatePeerID_values +\n",
        "               current_coordID_values        +\n",
        "               current_phase_value,\n",
        "               inplace=True,\n",
        "               axis=1)\n",
        "\n",
        "data_norm = pd.concat([data_norm, coordID_columns, estimatePeerID_colums, phase_colums], axis=1)\n",
        "data_np   = data_norm.to_numpy()\n",
        "\n",
        "delays        = []\n",
        "mut_variables = []\n",
        "\n",
        "for row in data_np:\n",
        "    splited_data    = np.split(row, [2 + number_of_nodes, 2 + (number_of_nodes*2)])\n",
        "    aggregated_data = np.concatenate((splited_data[0], splited_data[2]), axis=None)\n",
        "    \n",
        "    mut_variables.append([aggregated_data])\n",
        "    delays.append([splited_data[1]])\n",
        "\n",
        "delays = [[[ yNormalize(element, max_val_delay) for element in list] for list in matrix] for matrix in delays]\n",
        "\n",
        "delays        = np.asarray(delays)\n",
        "mut_variables = np.asarray(mut_variables)\n",
        "\n",
        "mut_variables_train = mut_variables[:train_set_size]\n",
        "mut_variables_test  = mut_variables[train_set_size:]\n",
        "\n",
        "delays_train = delays[:train_set_size]\n",
        "delays_test  = delays[train_set_size:]\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDfOEWysO6Ec",
        "colab_type": "code",
        "outputId": "b1bb7cb2-a908-4e39-a00a-76b861f0958d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "number_of_nodes          = 40\n",
        "algorithm_variables      = 5\n",
        "mutation                 = 'merge'\n",
        "matrix_rows, matrix_cols = 1, number_of_nodes*3 + 3\n",
        "\n",
        "filepath = 'sample_data/snapshots_' + mutation + '_' + str(number_of_nodes)  + '_2.csv'\n",
        "data     = load_data(filepath)\n",
        "\n",
        "train_set_size = int(len(data) * 0.8)\n",
        "\n",
        "#max_bandwidth  = data.Bandwidth.max()\n",
        "#min_bandwidth  = data.Bandwidth.min()\n",
        "#data.Bandwidth = data.Bandwidth.apply(xNormalize, args=(max_bandwidth, 0))\n",
        "\n",
        "data_np = data.to_numpy()\n",
        "delay   = data_np[:, (algorithm_variables + number_of_nodes):]\n",
        "\n",
        "max_val_delay = np.amax(delay)\n",
        "min_val_delay = np.amin(delay)\n",
        "\n",
        "data_norm = pd.get_dummies(data, \n",
        "                           columns=['Decision', 'PeerID', 'CoordID'],\n",
        "                           prefix=['Decision', 'PeerID', 'CoordID'])\n",
        "\n",
        "current_coordID_values = list(filter(lambda x: 'CoordID' in x, data_norm.columns.values))\n",
        "coordID_values         = get_coordID_values(number_of_nodes)\n",
        "coordID_columns        = data_norm.T.reindex(coordID_values).T.fillna(0)\n",
        "\n",
        "data_norm.drop(current_coordID_values, inplace=True, axis=1) \n",
        "\n",
        "data_norm = pd.concat([data_norm, coordID_columns], axis=1)\n",
        "data_norm = desnormalize(data_norm)\n",
        "data_np   = data_norm.to_numpy()\n",
        "\n",
        "delays        = []\n",
        "mut_variables = []\n",
        "\n",
        "for row in data_np:\n",
        "    splited_data    = np.split(row, [2 + number_of_nodes, 2 + (number_of_nodes*2)]) \n",
        "    aggregated_data = np.concatenate((splited_data[0], splited_data[2]), axis=None)\n",
        "    \n",
        "    mut_variables.append([aggregated_data])\n",
        "    delays.append([splited_data[1]])\n",
        "\n",
        "delays = [[[ yNormalize(element, max_val_delay) for element in list] for list in matrix] for matrix in delays]\n",
        "\n",
        "delays        = np.asarray(delays)\n",
        "mut_variables = np.asarray(mut_variables)\n",
        "\n",
        "mut_variables_train = mut_variables[:train_set_size]\n",
        "mut_variables_test  = mut_variables[train_set_size:]\n",
        "\n",
        "delays_train = delays[:train_set_size]\n",
        "delays_test  = delays[train_set_size:]\n",
        "\n",
        "\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KRBa_Ku95aa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_mat = data_norm.corr()\n",
        "fig   = plt.figure(figsize=(50,50))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = 1., square = True)\n",
        "#plt.show()\n",
        "plt.savefig('foo.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY3x37EccXAT",
        "colab_type": "text"
      },
      "source": [
        "## Train, compile and fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiFOtpKlbzya",
        "colab_type": "code",
        "outputId": "57b1140d-fdd1-407a-d245-de64618e1a1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!rm /content/Weights*.hdf5\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "batch_size = 16\n",
        "sess       = tf.Session()  \n",
        "\n",
        "K.set_session(sess)\n",
        "\n",
        "# Activation function\n",
        "activation = 'relu'\n",
        "\n",
        "# Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# First layer\n",
        "# The Input Layer :\n",
        "input_nodes = len(mut_variables_train[0][0])\n",
        "model.add(Dense(input_nodes, \n",
        "                kernel_initializer='normal', \n",
        "                input_shape=(1, matrix_cols), \n",
        "                activation=activation, \n",
        "                name=\"input_layer\"))\n",
        "\n",
        "# The Hidden Layers :\n",
        "model.add(Dense(128, kernel_initializer='normal',activation=activation))\n",
        "model.add(Dense(128, kernel_initializer='normal',activation=activation))\n",
        "model.add(Dense(128, kernel_initializer='normal',activation=activation))\n",
        "\n",
        "# The Output Layer :\n",
        "model.add(Dense(number_of_nodes, \n",
        "                kernel_initializer='normal', \n",
        "                activation='linear', \n",
        "                name=\"output_layer\"))\n",
        "\n",
        "# Compile the network :\n",
        "\n",
        "adam = Adam(lr=0.0001)\n",
        "#adam = 'adam'\n",
        "model.compile(loss='mean_absolute_error', \n",
        "              optimizer=adam, \n",
        "              metrics=['mean_absolute_error'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint      = ModelCheckpoint(checkpoint_name, \n",
        "                                  monitor='val_loss', \n",
        "                                  verbose=1, \n",
        "                                  save_best_only = True, \n",
        "                                  mode ='auto')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit \n",
        "model.fit(mut_variables_train, \n",
        "          delays_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=150,\n",
        "          validation_split=0.2,\n",
        "          callbacks=callbacks_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (Dense)          (None, 1, 123)            15252     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1, 128)            15872     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1, 128)            16512     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1, 128)            16512     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1, 40)             5160      \n",
            "=================================================================\n",
            "Total params: 69,308\n",
            "Trainable params: 69,308\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 44363 samples, validate on 11091 samples\n",
            "Epoch 1/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0596 - mean_absolute_error: 0.0596 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03717, saving model to Weights-001--0.03717.hdf5\n",
            "Epoch 2/150\n",
            "44363/44363 [==============================] - 9s 201us/step - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03717 to 0.03238, saving model to Weights-002--0.03238.hdf5\n",
            "Epoch 3/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0299 - val_mean_absolute_error: 0.0299\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03238 to 0.02990, saving model to Weights-003--0.02990.hdf5\n",
            "Epoch 4/150\n",
            "44363/44363 [==============================] - 9s 210us/step - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0279 - val_mean_absolute_error: 0.0279\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02990 to 0.02794, saving model to Weights-004--0.02794.hdf5\n",
            "Epoch 5/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0274 - mean_absolute_error: 0.0274 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02794 to 0.02750, saving model to Weights-005--0.02750.hdf5\n",
            "Epoch 6/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0262 - mean_absolute_error: 0.0262 - val_loss: 0.0266 - val_mean_absolute_error: 0.0266\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.02750 to 0.02655, saving model to Weights-006--0.02655.hdf5\n",
            "Epoch 7/150\n",
            "44363/44363 [==============================] - 9s 204us/step - loss: 0.0253 - mean_absolute_error: 0.0253 - val_loss: 0.0268 - val_mean_absolute_error: 0.0268\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.02655\n",
            "Epoch 8/150\n",
            "44363/44363 [==============================] - 9s 203us/step - loss: 0.0245 - mean_absolute_error: 0.0245 - val_loss: 0.0245 - val_mean_absolute_error: 0.0245\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02655 to 0.02451, saving model to Weights-008--0.02451.hdf5\n",
            "Epoch 9/150\n",
            "44363/44363 [==============================] - 9s 207us/step - loss: 0.0240 - mean_absolute_error: 0.0240 - val_loss: 0.0239 - val_mean_absolute_error: 0.0239\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.02451 to 0.02392, saving model to Weights-009--0.02392.hdf5\n",
            "Epoch 10/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0236 - mean_absolute_error: 0.0236 - val_loss: 0.0234 - val_mean_absolute_error: 0.0234\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.02392 to 0.02342, saving model to Weights-010--0.02342.hdf5\n",
            "Epoch 11/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0232 - mean_absolute_error: 0.0232 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02342\n",
            "Epoch 12/150\n",
            "44363/44363 [==============================] - 9s 210us/step - loss: 0.0229 - mean_absolute_error: 0.0229 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02342\n",
            "Epoch 13/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0226 - mean_absolute_error: 0.0226 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.02342 to 0.02281, saving model to Weights-013--0.02281.hdf5\n",
            "Epoch 14/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0223 - mean_absolute_error: 0.0223 - val_loss: 0.0218 - val_mean_absolute_error: 0.0218\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.02281 to 0.02184, saving model to Weights-014--0.02184.hdf5\n",
            "Epoch 15/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0223 - mean_absolute_error: 0.0223 - val_loss: 0.0228 - val_mean_absolute_error: 0.0228\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02184\n",
            "Epoch 16/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0219 - mean_absolute_error: 0.0219 - val_loss: 0.0223 - val_mean_absolute_error: 0.0223\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02184\n",
            "Epoch 17/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0220 - mean_absolute_error: 0.0220 - val_loss: 0.0210 - val_mean_absolute_error: 0.0210\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.02184 to 0.02101, saving model to Weights-017--0.02101.hdf5\n",
            "Epoch 18/150\n",
            "44363/44363 [==============================] - 9s 209us/step - loss: 0.0217 - mean_absolute_error: 0.0217 - val_loss: 0.0217 - val_mean_absolute_error: 0.0217\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.02101\n",
            "Epoch 19/150\n",
            "44363/44363 [==============================] - 9s 208us/step - loss: 0.0214 - mean_absolute_error: 0.0214 - val_loss: 0.0210 - val_mean_absolute_error: 0.0210\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.02101 to 0.02101, saving model to Weights-019--0.02101.hdf5\n",
            "Epoch 20/150\n",
            " 9792/44363 [=====>........................] - ETA: 6s - loss: 0.0216 - mean_absolute_error: 0.0216"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_eibqJ1oyNp",
        "colab_type": "code",
        "outputId": "eaeed9fc-26bd-4965-d078-6c7ef26e11a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "from os      import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "path  = '/content/'\n",
        "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "\n",
        "weights_file = 'Weights-999--9.99999.hdf5' \n",
        "min_value    = 1000.0\n",
        "\n",
        "for f in files:\n",
        "    m = re.search('Weights-\\d\\d\\d--(\\d+\\.\\d+).hdf5', f)\n",
        "    if m:\n",
        "        value = float(m.group(1))\n",
        "        if value < min_value:\n",
        "            min_value = value\n",
        "            weights_file = f\n",
        "        \n",
        "print(weights_file)\n",
        "    \n",
        "model.load_weights(weights_file)  \n",
        "model.compile(loss='mean_absolute_error', \n",
        "              optimizer=Adam(lr=0.0001), \n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "print('Done.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights-150--0.01213.hdf5\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXfnyK3ltoIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get input and output layer name\n",
        "for n in sess.graph.as_graph_def().node:\n",
        "    if 'input_layer' in n.name:\n",
        "          print(n.name)\n",
        "    if 'output_layer' in n.name:\n",
        "          print(n.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6fMscnKnyP_",
        "colab_type": "text"
      },
      "source": [
        "### **Evaluate predicitons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzg_WmHwROjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(mut_variables_test)\n",
        "\n",
        "for i in range(60,90):\n",
        "    print(mut_variables_test[i][0])\n",
        "    print(delays_test[i][0])\n",
        "    print(predictions[i][0])\n",
        "    print(\"----------\" + str(i))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QXewADnsNR",
        "colab_type": "text"
      },
      "source": [
        "### **Test Centralized**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wreIq9nF0QSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bandwidth = 500\n",
        "\n",
        "test_data = [\n",
        "    [\n",
        "        [0,bandwidth,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [maj,bandwidth,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,vote,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ]\n",
        "]\n",
        "\n",
        "predictions = model.predict(np.asarray(test_data))\n",
        "\n",
        "l = [2, 1, 2, 2, 2, 2]\n",
        "i = 0\n",
        "\n",
        "for li in l:\n",
        "    print(predictions[i][0])\n",
        "        \n",
        "    print(\"- PEER \" + str(li))\n",
        "    i += 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGnE_Ze0nhLt",
        "colab_type": "text"
      },
      "source": [
        "### **Test Ring**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM6FFadWIfyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bandwidth = 100\n",
        "\n",
        "test_data = [\n",
        "    [\n",
        "        [0,bandwidth,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [0,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [ \n",
        "        [maj,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [maj,bandwidth,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,vote,vote,0,0,0,0,0,0,0,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ],\n",
        "    [\n",
        "        [maj,bandwidth,0,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,vote,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,peerID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,coordID,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    ]\n",
        "]\n",
        "\n",
        "predictions = model.predict(np.asarray(test_data))\n",
        "\n",
        "l = [2, 3, 1, 4, 40, 3, 1, 39, 15, 16, 17, 18, 19, 20, 21, 22, 31, 5]\n",
        "i = 0\n",
        "for li in l:\n",
        "    if li < 35 and li > 5:\n",
        "        print(predictions[i][0][(li - 2):(li + 2)])\n",
        "    elif li <= 5:\n",
        "        print(predictions[i][0][38:], end='')\n",
        "        print(predictions[i][0][:(li + 2)])\n",
        "    else:\n",
        "        print(predictions[i][0][(li - 2):], end='')\n",
        "        print(predictions[i][0][:2])\n",
        "        \n",
        "    print(\"- PEER \" + str(li))\n",
        "    i += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBLSj_kCV3ID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use TF to save the graph model instead of Keras save model to load it in Golang\n",
        "builder = tf.saved_model.builder.SavedModelBuilder(\"/content/drive/My Drive/datasets/mut_model_merge_40_5\")  \n",
        "# Tag the model, required for Go\n",
        "builder.add_meta_graph_and_variables(sess, [\"mut_tag\"], clear_devices=True)  \n",
        "builder.save()\n",
        "# sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}